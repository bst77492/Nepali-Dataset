{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing File and Kappa Score\n"
      ],
      "metadata": {
        "id": "xe6RBzdZTMdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load the first sheet (index 0) or by name if it's called \"Annotation\"\n",
        "dfw = pd.read_excel(\"/content/Final_Annotation_Updated.xlsx\", sheet_name=0)\n",
        "\n",
        "# Preview\n",
        "print(dfw.head(2))\n",
        "\n",
        "# Drop missing values in annotator columns\n",
        "dfw = dfw.dropna(subset=[\"Annotator_1_Sentiment\", \"Annotator_11_Sentiment\"])\n",
        "\n",
        "# Calculate Cohen's Kappa\n",
        "kappa = cohen_kappa_score(dfw[\"Annotator_1_Sentiment\"], dfw[\"Annotator_11_Sentiment\"])\n",
        "\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")"
      ],
      "metadata": {
        "id": "B0AqR4yZTQ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Run"
      ],
      "metadata": {
        "id": "NHvrMEO4TVLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "%pip install evaluate -qqq\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# -----------------------\n",
        "# 1. Data Preparation\n",
        "# -----------------------\n",
        "df = pd.read_excel(\"/content/Final_Annotation.xlsx\")\n",
        "df = df.dropna(subset=[\"Final\"])  # Drop rows with missing sentiment labels\n",
        "df[\"label\"] = df[\"Final\"].map({\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2})\n",
        "df = df[[\"Sentence\", \"label\"]]\n",
        "\n",
        "# Print class distribution for debugging purposes.\n",
        "print(\"Full dataset label distribution:\")\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(\"\\nTraining set label distribution:\")\n",
        "print(train_df[\"label\"].value_counts())\n",
        "print(\"\\nTest set label distribution:\")\n",
        "print(test_df[\"label\"].value_counts())\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# -----------------------\n",
        "# 2. Tokenization\n",
        "# -----------------------\n",
        "model_name = \"xlm-roberta-base\"  # or your chosen model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"Sentence\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# -----------------------\n",
        "# 3. Model & Metric Setup\n",
        "# -----------------------\n",
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# -----------------------\n",
        "# 4. Training Arguments (Modified)\n",
        "# -----------------------\n",
        "# We remove early stopping and best-model loading to force full training, and add overwrite_output_dir.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./best_model_xlmroberta\",\n",
        "    overwrite_output_dir=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    num_train_epochs=11,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    label_smoothing_factor=0.1,\n",
        "    # Removed load_best_model_at_end and early stopping for full epoch training.\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 5. Trainer Setup and Full Training\n",
        "# -----------------------\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# -----------------------\n",
        "# 6. Evaluation and Error Analysis\n",
        "# -----------------------\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nFinal evaluation results:\", eval_results)\n",
        "\n",
        "# Get predictions on the test set.\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# Confusion Matrix.\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
        "\n",
        "# Error Analysis: Print misclassified examples.\n",
        "test_df_predictions = test_dataset.to_pandas()\n",
        "test_df_predictions[\"predicted\"] = y_pred\n",
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "test_df_predictions[\"true_label_str\"] = test_df_predictions[\"labels\"].map(label_map)\n",
        "test_df_predictions[\"predicted_str\"] = test_df_predictions[\"predicted\"].map(label_map)\n",
        "\n",
        "misclassified = test_df_predictions[test_df_predictions[\"labels\"] != test_df_predictions[\"predicted\"]]\n",
        "print(\"\\nSome misclassified examples:\")\n",
        "print(misclassified[[\"Sentence\", \"true_label_str\", \"predicted_str\"]].head(10))\n",
        "misclassified.to_csv('misclassified.csv',encoding='utf-8')\n",
        "\n",
        "# -----------------------\n",
        "# 7. Interactive Testing\n",
        "# -----------------------\n",
        "new_sentence = input(\"\\nEnter a new sentence to test: \")\n",
        "inputs = tokenizer(new_sentence, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
        "# Ensure the inputs are on the same device as the model (e.g., cuda:0)\n",
        "inputs = {key: value.to(trainer.model.device) for key, value in inputs.items()}\n",
        "outputs = trainer.model(**inputs)\n",
        "# Apply softmax to get probabilities.\n",
        "probs = outputs.logits.softmax(dim=-1)\n",
        "pred_class = int(probs.argmax(dim=-1).item())\n",
        "print(f\"\\nPredicted sentiment: {label_map[pred_class]}\")\n",
        "print(f\"Probabilities: {probs.detach().cpu().numpy()}\")"
      ],
      "metadata": {
        "id": "8rasost2TbZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Run + Stopwords"
      ],
      "metadata": {
        "id": "LJYjYeyyTm6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "%pip install evaluate -qqq\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# -----------------------\n",
        "# 1. Data Preparation\n",
        "# -----------------------\n",
        "df = pd.read_excel(\"/content/Final_Annotation_Updated.xlsx\")\n",
        "df = df.dropna(subset=[\"Final\"])  # Drop rows with missing sentiment labels\n",
        "df[\"label\"] = df[\"Final\"].map({\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2})\n",
        "df = df[[\"Sentence\", \"label\"]]\n",
        "\n",
        "# Print class distribution for debugging purposes.\n",
        "print(\"Full dataset label distribution:\")\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(\"\\nTraining set label distribution:\")\n",
        "print(train_df[\"label\"].value_counts())\n",
        "print(\"\\nTest set label distribution:\")\n",
        "print(test_df[\"label\"].value_counts())\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# -----------------------\n",
        "# 2. Tokenization\n",
        "# -----------------------\n",
        "model_name = \"xlm-roberta-base\"  # or your chosen model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"Sentence\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# -----------------------\n",
        "# 3. Model & Metric Setup\n",
        "# -----------------------\n",
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# -----------------------\n",
        "# 4. Training Arguments (Modified)\n",
        "# -----------------------\n",
        "# We remove early stopping and best-model loading to force full training, and add overwrite_output_dir.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./best_model_xlmroberta\",\n",
        "    overwrite_output_dir=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    num_train_epochs=11,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    label_smoothing_factor=0.1,\n",
        "    # Removed load_best_model_at_end and early stopping for full epoch training.\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 5. Trainer Setup and Full Training\n",
        "# -----------------------\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# -----------------------\n",
        "# 6. Evaluation and Error Analysis\n",
        "# -----------------------\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nFinal evaluation results:\", eval_results)\n",
        "\n",
        "# Get predictions on the test set.\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# Confusion Matrix.\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
        "\n",
        "# Error Analysis: Print misclassified examples.\n",
        "test_df_predictions = test_dataset.to_pandas()\n",
        "test_df_predictions[\"predicted\"] = y_pred\n",
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "test_df_predictions[\"true_label_str\"] = test_df_predictions[\"labels\"].map(label_map)\n",
        "test_df_predictions[\"predicted_str\"] = test_df_predictions[\"predicted\"].map(label_map)\n",
        "\n",
        "misclassified = test_df_predictions[test_df_predictions[\"labels\"] != test_df_predictions[\"predicted\"]]\n",
        "print(\"\\nSome misclassified examples:\")\n",
        "print(misclassified[[\"Sentence\", \"true_label_str\", \"predicted_str\"]].head(10))\n",
        "misclassified.to_csv('misclassified.csv',encoding='utf-8')\n",
        "\n",
        "# -----------------------\n",
        "# 7. Interactive Testing\n",
        "# -----------------------\n",
        "new_sentence = input(\"\\nEnter a new sentence to test: \")\n",
        "inputs = tokenizer(new_sentence, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
        "# Ensure the inputs are on the same device as the model (e.g., cuda:0)\n",
        "inputs = {key: value.to(trainer.model.device) for key, value in inputs.items()}\n",
        "outputs = trainer.model(**inputs)\n",
        "# Apply softmax to get probabilities.\n",
        "probs = outputs.logits.softmax(dim=-1)\n",
        "pred_class = int(probs.argmax(dim=-1).item())\n",
        "print(f\"\\nPredicted sentiment: {label_map[pred_class]}\")\n",
        "print(f\"Probabilities: {probs.detach().cpu().numpy()}\")"
      ],
      "metadata": {
        "id": "06fJgiRITpaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}